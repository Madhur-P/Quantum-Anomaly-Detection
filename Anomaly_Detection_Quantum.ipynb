{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install qiskit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "013vqKqmbv51",
        "outputId": "859fc9bf-c1f7-4e9f-a418-3d0b8d0a34e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qiskit in /usr/local/lib/python3.12/dist-packages (1.4.5)\n",
            "Requirement already satisfied: rustworkx>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from qiskit) (0.17.1)\n",
            "Requirement already satisfied: numpy<3,>=1.17 in /usr/local/lib/python3.12/dist-packages (from qiskit) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.12/dist-packages (from qiskit) (1.15.3)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.12/dist-packages (from qiskit) (1.13.3)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.12/dist-packages (from qiskit) (0.3.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from qiskit) (2.9.0.post0)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from qiskit) (5.5.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from qiskit) (4.15.0)\n",
            "Requirement already satisfied: symengine<0.14,>=0.11 in /usr/local/lib/python3.12/dist-packages (from qiskit) (0.13.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.0->qiskit) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.3->qiskit) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qiskit-aer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCka_0lSbz7M",
        "outputId": "96224210-4f7f-4ada-b194-1c2f142d9438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qiskit-aer in /usr/local/lib/python3.12/dist-packages (0.17.2)\n",
            "Requirement already satisfied: qiskit>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from qiskit-aer) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.12/dist-packages (from qiskit-aer) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.12/dist-packages (from qiskit-aer) (1.15.3)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.12/dist-packages (from qiskit-aer) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from qiskit-aer) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.0->qiskit-aer) (1.17.0)\n",
            "Requirement already satisfied: rustworkx>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.1.0->qiskit-aer) (0.17.1)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.1.0->qiskit-aer) (1.13.3)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.1.0->qiskit-aer) (0.3.8)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.1.0->qiskit-aer) (5.5.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.1.0->qiskit-aer) (4.15.0)\n",
            "Requirement already satisfied: symengine<0.14,>=0.11 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.1.0->qiskit-aer) (0.13.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.3->qiskit>=1.1.0->qiskit-aer) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qiskit-machine-learning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJr071MOcC60",
        "outputId": "12f11b09-3e62-432e-8ced-65fbd1929c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qiskit-machine-learning in /usr/local/lib/python3.12/dist-packages (0.8.4)\n",
            "Requirement already satisfied: qiskit<2.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from qiskit-machine-learning) (1.4.5)\n",
            "Requirement already satisfied: numpy>=2.0 in /usr/local/lib/python3.12/dist-packages (from qiskit-machine-learning) (2.0.2)\n",
            "Requirement already satisfied: scipy<1.16,>=1.4 in /usr/local/lib/python3.12/dist-packages (from qiskit-machine-learning) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.2 in /usr/local/lib/python3.12/dist-packages (from qiskit-machine-learning) (1.6.1)\n",
            "Requirement already satisfied: setuptools>=40.1 in /usr/local/lib/python3.12/dist-packages (from qiskit-machine-learning) (75.2.0)\n",
            "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.12/dist-packages (from qiskit-machine-learning) (0.3.8)\n",
            "Requirement already satisfied: rustworkx>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from qiskit<2.0,>=1.0->qiskit-machine-learning) (0.17.1)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.12/dist-packages (from qiskit<2.0,>=1.0->qiskit-machine-learning) (1.13.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from qiskit<2.0,>=1.0->qiskit-machine-learning) (2.9.0.post0)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from qiskit<2.0,>=1.0->qiskit-machine-learning) (5.5.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from qiskit<2.0,>=1.0->qiskit-machine-learning) (4.15.0)\n",
            "Requirement already satisfied: symengine<0.14,>=0.11 in /usr/local/lib/python3.12/dist-packages (from qiskit<2.0,>=1.0->qiskit-machine-learning) (0.13.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2->qiskit-machine-learning) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2->qiskit-machine-learning) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.0->qiskit<2.0,>=1.0->qiskit-machine-learning) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.3->qiskit<2.0,>=1.0->qiskit-machine-learning) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Imports and user-tunable parameters\n",
        "import os, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Qiskit\n",
        "from qiskit_aer import Aer\n",
        "from qiskit.circuit.library import ZZFeatureMap\n",
        "from qiskit_machine_learning.kernels import FidelityStatevectorKernel\n",
        "\n",
        "# Repro\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ----- Tunable parameters -----\n",
        "PCA_COMPONENTS = 12        # number of PCA components = number of qubits\n",
        "QC_REPS = 2                # reps for ZZFeatureMap\n",
        "NUM_CLASSES = 4            # total classes including 'normal' (e.g., normal + top-3 attacks)\n",
        "PER_CLASS = 400            # training samples per class (adjust to control runtime)\n",
        "TEST_SIZE = 1000            # number of test samples to evaluate\n",
        "# ------------------------------\n",
        "print(\"Parameters: PCA_COMPONENTS =\", PCA_COMPONENTS, \"NUM_CLASSES =\", NUM_CLASSES,\n",
        "      \"PER_CLASS =\", PER_CLASS, \"TEST_SIZE =\", TEST_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzFfRhIfc_A6",
        "outputId": "5e823331-e5e4-4aed-f4d2-3a06545e2d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters: PCA_COMPONENTS = 12 NUM_CLASSES = 4 PER_CLASS = 400 TEST_SIZE = 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Load preprocessed PCA arrays if present, else do preprocessing from raw KDD files\n",
        "# It produces: df_train, df_test, X_train_pca, X_test_pca (and label arrays)\n",
        "\n",
        "# Filenames\n",
        "TRAIN_RAW = \"/bin/KDD_Dataset/KDDTrain+.txt\"\n",
        "TEST_RAW  = \"/bin/KDD_Dataset/KDDTest+.txt\"\n",
        "SAVE_XTRAIN = \"/bin/KDD_Dataset/X_train_pca.npy\"\n",
        "SAVE_XTEST  = \"/bin/KDD_Dataset/X_test_pca.npy\"\n",
        "SAVE_DFTRAIN = \"/bin/KDD_Dataset/df_train.pkl\"\n",
        "SAVE_DFTEST  = \"/bin/KDD_Dataset/df_test.pkl\"\n",
        "\n",
        "if os.path.exists(SAVE_XTRAIN) and os.path.exists(SAVE_XTEST) and os.path.exists(SAVE_DFTRAIN) and os.path.exists(SAVE_DFTEST):\n",
        "    print(\"Found saved PCA arrays and dataframes — loading them.\")\n",
        "    X_train_pca = np.load(SAVE_XTRAIN)\n",
        "    X_test_pca  = np.load(SAVE_XTEST)\n",
        "    df_train = pd.read_pickle(SAVE_DFTRAIN)\n",
        "    df_test  = pd.read_pickle(SAVE_DFTEST)\n",
        "    print(\"Loaded:\", X_train_pca.shape, X_test_pca.shape, \"df_train:\", df_train.shape)\n",
        "else:\n",
        "    # Preprocess from raw files\n",
        "    print(\"Saved arrays not found — preprocessing from raw NSL-KDD files. This may take a few minutes.\")\n",
        "    columns = [\n",
        "        'duration','protocol_type','service','flag','src_bytes','dst_bytes',\n",
        "        'land','wrong_fragment','urgent','hot','num_failed_logins','logged_in',\n",
        "        'num_compromised','root_shell','su_attempted','num_root','num_file_creations',\n",
        "        'num_shells','num_access_files','num_outbound_cmds','is_host_login','is_guest_login',\n",
        "        'count','srv_count','serror_rate','srv_serror_rate','rerror_rate','srv_rerror_rate',\n",
        "        'same_srv_rate','diff_srv_rate','srv_diff_host_rate','dst_host_count','dst_host_srv_count',\n",
        "        'dst_host_same_srv_rate','dst_host_diff_srv_rate','dst_host_same_src_port_rate',\n",
        "        'dst_host_srv_diff_host_rate','dst_host_serror_rate','dst_host_srv_serror_rate',\n",
        "        'dst_host_rerror_rate','dst_host_srv_rerror_rate','label','difficulty'\n",
        "    ]\n",
        "    # Load raw text files\n",
        "    df_train = pd.read_csv(TRAIN_RAW, names=columns)\n",
        "    df_test  = pd.read_csv(TEST_RAW,  names=columns)\n",
        "    # Clean labels: string type, strip, lowercase, rstrip('.')\n",
        "    df_train['label'] = df_train['label'].astype(str).str.strip().str.lower().str.rstrip('.')\n",
        "    df_test['label']  = df_test['label'].astype(str).str.strip().str.lower().str.rstrip('.')\n",
        "    # Drop difficulty (we won't use it)\n",
        "    df_train_proc = df_train.drop(columns=['difficulty']).copy()\n",
        "    df_test_proc  = df_test.drop(columns=['difficulty']).copy()\n",
        "    # One-hot encode categorical columns (detect objects except label)\n",
        "    cat_cols = df_train_proc.select_dtypes(include=['object']).columns.tolist()\n",
        "    cat_cols = [c for c in cat_cols if c != 'label']\n",
        "    print(\"Categorical columns detected:\", cat_cols)\n",
        "    df_train_proc = pd.get_dummies(df_train_proc, columns=cat_cols)\n",
        "    df_test_proc  = pd.get_dummies(df_test_proc,  columns=cat_cols)\n",
        "    # Align test to train columns\n",
        "    df_test_proc = df_test_proc.reindex(columns=df_train_proc.columns, fill_value=0)\n",
        "    # Separate labels and features\n",
        "    y_train_all = df_train_proc['label'].astype(str).values\n",
        "    y_test_all  = df_test_proc['label'].astype(str).values\n",
        "    X_train_df = df_train_proc.drop(columns=['label']).copy()\n",
        "    X_test_df  = df_test_proc.drop(columns=['label']).copy()\n",
        "    # Scale\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_df)\n",
        "    X_test_scaled  = scaler.transform(X_test_df)\n",
        "    # PCA\n",
        "    from sklearn.decomposition import PCA\n",
        "    pca = PCA(n_components=PCA_COMPONENTS, random_state=SEED)\n",
        "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "    X_test_pca  = pca.transform(X_test_scaled)\n",
        "    # Save for reuse\n",
        "    np.save(SAVE_XTRAIN, X_train_pca)\n",
        "    np.save(SAVE_XTEST, X_test_pca)\n",
        "    df_train = df_train  # keep original df_train (with difficulty), but df_train_proc exists too\n",
        "    df_test = df_test\n",
        "    df_train.to_pickle(SAVE_DFTRAIN)\n",
        "    df_test.to_pickle(SAVE_DFTEST)\n",
        "    print(\"Preprocessing complete. PCA shapes:\", X_train_pca.shape, X_test_pca.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hat5h6XIfbSU",
        "outputId": "2699ae90-a123-4aa7-e752-a8ce71352837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found saved PCA arrays and dataframes — loading them.\n",
            "Loaded: (125973, 12) (22544, 12) df_train: (125973, 43)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Create quantum feature map and FidelityStatevectorKernel\n",
        "num_qubits = X_train_pca.shape[1]\n",
        "print(\"Using\", num_qubits, \"qubits (PCA components).\")\n",
        "\n",
        "feature_map = ZZFeatureMap(feature_dimension=num_qubits, reps=QC_REPS, entanglement='linear')\n",
        "quantum_kernel = FidelityStatevectorKernel(feature_map=feature_map)  # no quantum_instance argument for this version\n",
        "\n",
        "print(\"Feature map and fidelity kernel initialized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1e7wqb_fi4k",
        "outputId": "2c6b4cfa-e241-4b35-87b7-8e6ca990d4e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 12 qubits (PCA components).\n",
            "Feature map and fidelity kernel initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Select classes (include 'normal') and build a balanced training subset\n",
        "# Get label array from df_train (cleaned)\n",
        "labels_all = df_train['label'].astype(str).str.strip().str.lower().to_numpy()\n",
        "unique, counts = np.unique(labels_all, return_counts=True)\n",
        "label_counts = sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)\n",
        "print(\"Top label counts (train):\", label_counts[:10])\n",
        "\n",
        "# Build list of classes: always include 'normal' if present, then top attacks\n",
        "classes = ['normal'] if 'normal' in unique else []\n",
        "for lbl, _ in label_counts:\n",
        "    if lbl == 'normal': continue\n",
        "    if len(classes) >= NUM_CLASSES:\n",
        "        break\n",
        "    classes.append(lbl)\n",
        "classes = classes[:NUM_CLASSES]\n",
        "print(\"Selected classes for experiment:\", classes)\n",
        "\n",
        "# Collect up to PER_CLASS samples per class\n",
        "np.random.seed(SEED)\n",
        "indices_per_class = []\n",
        "for cls in classes:\n",
        "    idxs = np.where(labels_all == cls)[0]\n",
        "    if len(idxs) == 0:\n",
        "        raise RuntimeError(f\"No samples found for class '{cls}'\")\n",
        "    take = min(PER_CLASS, len(idxs))\n",
        "    chosen = np.random.choice(idxs, take, replace=False)\n",
        "    indices_per_class.append(chosen)\n",
        "\n",
        "train_idx = np.concatenate(indices_per_class)\n",
        "X_train_multi = X_train_pca[train_idx]\n",
        "y_train_multi = labels_all[train_idx]\n",
        "\n",
        "print(\"Training multiclass subset shape:\", X_train_multi.shape)\n",
        "print(\"Per-class counts:\", Counter(y_train_multi))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtQOVt6efqYF",
        "outputId": "86a90ce9-3e21-4e4c-e016-54ffa5bb1f98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top label counts (train): [('normal', np.int64(67343)), ('neptune', np.int64(41214)), ('satan', np.int64(3633)), ('ipsweep', np.int64(3599)), ('portsweep', np.int64(2931)), ('smurf', np.int64(2646)), ('nmap', np.int64(1493)), ('back', np.int64(956)), ('teardrop', np.int64(892)), ('warezclient', np.int64(890))]\n",
            "Selected classes for experiment: ['normal', 'neptune', 'satan', 'ipsweep']\n",
            "Training multiclass subset shape: (1600, 12)\n",
            "Per-class counts: Counter({'normal': 400, 'neptune': 400, 'satan': 400, 'ipsweep': 400})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Compute quantum training kernel K_train and train One-vs-Rest SVM\n",
        "t0 = time.time()\n",
        "print(\"Computing quantum kernel matrix K_train for training subset (may take time)...\")\n",
        "K_train = quantum_kernel.evaluate(x_vec=X_train_multi)   # shape (n_train, n_train)\n",
        "t1 = time.time()\n",
        "print(f\"K_train shape: {K_train.shape}  computed in {t1-t0:.1f} s\")\n",
        "\n",
        "# Train One-vs-Rest SVM with precomputed kernel\n",
        "print(\"Training One-vs-Rest SVM with precomputed kernel...\")\n",
        "t2 = time.time()\n",
        "base = SVC(kernel='precomputed')\n",
        "ovr = OneVsRestClassifier(base)\n",
        "ovr.fit(K_train, y_train_multi)\n",
        "t3 = time.time()\n",
        "print(f\"Trained One-vs-Rest in {t3-t2:.1f} s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPz45ZYafter",
        "outputId": "5b505fa2-f230-45ff-9fec-058972a428d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing quantum kernel matrix K_train for training subset (may take time)...\n",
            "K_train shape: (1600, 1600)  computed in 95.2 s\n",
            "Training One-vs-Rest SVM with precomputed kernel...\n",
            "Trained One-vs-Rest in 0.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Prepare test set (first TEST_SIZE samples or stratified sample) and evaluate\n",
        "# Use first TEST_SIZE test samples by default\n",
        "n_test_available = X_test_pca.shape[0]\n",
        "test_n = min(TEST_SIZE, n_test_available)\n",
        "X_test_eval = X_test_pca[:test_n]\n",
        "y_test_labels = df_test['label'].astype(str).str.strip().str.lower().to_numpy()[:test_n]\n",
        "\n",
        "print(\"Evaluating on test_n =\", test_n, \"samples. Classes considered:\", classes)\n",
        "\n",
        "t4 = time.time()\n",
        "print(\"Computing K_test (between test samples and training subset)...\")\n",
        "K_test = quantum_kernel.evaluate(x_vec=X_test_eval, y_vec=X_train_multi)   # (n_test, n_train)\n",
        "t5 = time.time()\n",
        "print(f\"K_test shape: {K_test.shape}  computed in {t5-t4:.1f} s\")\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = ovr.predict(K_test)\n",
        "print(\"\\nConfusion Matrix (rows=actual, cols=predicted) for selected classes:\")\n",
        "print(confusion_matrix(y_test_labels, y_pred, labels=classes))\n",
        "\n",
        "print(\"\\nClassification Report (for selected classes):\")\n",
        "print(classification_report(y_test_labels, y_pred, labels=classes, zero_division=0))\n",
        "\n",
        "# Overall accuracy on these test_n samples (labels outside 'classes' will be counted as \"other/mismatch\")\n",
        "acc = accuracy_score(y_test_labels, y_pred)\n",
        "print(f\"\\nOverall accuracy (on {test_n} test samples): {acc:.4f}\")\n",
        "\n",
        "# Print timings summary\n",
        "print(\"\\nTimings (s): K_train =\", t1-t0, \"train_svm =\", t3-t2, \"K_test =\", t5-t4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdBAJTMEfxjE",
        "outputId": "4829858f-144d-439a-9060-d4d0b4c1f2ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on test_n = 1000 samples. Classes considered: ['normal', 'neptune', 'satan', 'ipsweep']\n",
            "Computing K_test (between test samples and training subset)...\n",
            "K_test shape: (1000, 1600)  computed in 80.7 s\n",
            "\n",
            "Confusion Matrix (rows=actual, cols=predicted) for selected classes:\n",
            "[[449   0   3   0]\n",
            " [ 18 196   1   0]\n",
            " [ 12   0  21   0]\n",
            " [  0   0   0   6]]\n",
            "\n",
            "Classification Report (for selected classes):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.59      0.99      0.74       452\n",
            "     neptune       1.00      0.91      0.95       215\n",
            "       satan       0.51      0.64      0.57        33\n",
            "     ipsweep       1.00      1.00      1.00         6\n",
            "\n",
            "   micro avg       0.67      0.95      0.79       706\n",
            "   macro avg       0.78      0.89      0.82       706\n",
            "weighted avg       0.72      0.95      0.80       706\n",
            "\n",
            "\n",
            "Overall accuracy (on 1000 test samples): 0.6720\n",
            "\n",
            "Timings (s): K_train = 95.22372436523438 train_svm = 0.21852850914001465 K_test = 80.69538807868958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Optional save of kernel / indices / predictions (only if size reasonable)\n",
        "save_dir = \"quantum_multiclass_results\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save training indices and classes\n",
        "np.save(os.path.join(save_dir, \"train_idx.npy\"), train_idx)\n",
        "with open(os.path.join(save_dir, \"classes.txt\"), \"w\") as f:\n",
        "    f.write(\"\\n\".join(classes))\n",
        "\n",
        "# Save small kernel/results (be cautious if matrices are large)\n",
        "np.save(os.path.join(save_dir, \"K_train.npy\"), K_train)\n",
        "np.save(os.path.join(save_dir, \"K_test.npy\"), K_test)\n",
        "pd.DataFrame({\"y_test\": y_test_labels, \"y_pred\": y_pred}).to_csv(os.path.join(save_dir, \"predictions.csv\"), index=False)\n",
        "\n",
        "print(\"Saved results to\", save_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYqqhd5GgArL",
        "outputId": "2f26586a-2f4e-4d09-cde5-5eb857a64b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved results to quantum_multiclass_results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FBF75fUEiYOr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}