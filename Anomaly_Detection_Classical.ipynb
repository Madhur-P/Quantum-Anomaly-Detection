{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKOrGhrsjlPZ",
        "outputId": "708589d3-8265-48e6-dc1d-cd9de4a421d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qiskit\n",
            "  Downloading qiskit-2.2.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (12 kB)\n",
            "Collecting rustworkx>=0.15.0 (from qiskit)\n",
            "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.17 in /usr/local/lib/python3.12/dist-packages (from qiskit) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.12/dist-packages (from qiskit) (1.16.3)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.12/dist-packages (from qiskit) (0.3.8)\n",
            "Collecting stevedore>=3.0.0 (from qiskit)\n",
            "  Downloading stevedore-5.5.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from qiskit) (4.15.0)\n",
            "Downloading qiskit-2.2.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stevedore-5.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: stevedore, rustworkx, qiskit\n",
            "Successfully installed qiskit-2.2.3 rustworkx-0.17.1 stevedore-5.5.0\n",
            "Collecting qiskit-aer\n",
            "  Downloading qiskit_aer-0.17.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: qiskit>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from qiskit-aer) (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.12/dist-packages (from qiskit-aer) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.12/dist-packages (from qiskit-aer) (1.16.3)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.12/dist-packages (from qiskit-aer) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from qiskit-aer) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.0->qiskit-aer) (1.17.0)\n",
            "Requirement already satisfied: rustworkx>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.1.0->qiskit-aer) (0.17.1)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.1.0->qiskit-aer) (0.3.8)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.1.0->qiskit-aer) (5.5.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.1.0->qiskit-aer) (4.15.0)\n",
            "Downloading qiskit_aer-0.17.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: qiskit-aer\n",
            "Successfully installed qiskit-aer-0.17.2\n",
            "Collecting qiskit-machine-learning\n",
            "  Downloading qiskit_machine_learning-0.8.4-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting qiskit<2.0,>=1.0 (from qiskit-machine-learning)\n",
            "  Downloading qiskit-1.4.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=2.0 in /usr/local/lib/python3.12/dist-packages (from qiskit-machine-learning) (2.0.2)\n",
            "Collecting scipy<1.16,>=1.4 (from qiskit-machine-learning)\n",
            "  Downloading scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=1.2 in /usr/local/lib/python3.12/dist-packages (from qiskit-machine-learning) (1.6.1)\n",
            "Requirement already satisfied: setuptools>=40.1 in /usr/local/lib/python3.12/dist-packages (from qiskit-machine-learning) (75.2.0)\n",
            "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.12/dist-packages (from qiskit-machine-learning) (0.3.8)\n",
            "Requirement already satisfied: rustworkx>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from qiskit<2.0,>=1.0->qiskit-machine-learning) (0.17.1)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.12/dist-packages (from qiskit<2.0,>=1.0->qiskit-machine-learning) (1.13.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from qiskit<2.0,>=1.0->qiskit-machine-learning) (2.9.0.post0)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from qiskit<2.0,>=1.0->qiskit-machine-learning) (5.5.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from qiskit<2.0,>=1.0->qiskit-machine-learning) (4.15.0)\n",
            "Collecting symengine<0.14,>=0.11 (from qiskit<2.0,>=1.0->qiskit-machine-learning)\n",
            "  Downloading symengine-0.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2->qiskit-machine-learning) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2->qiskit-machine-learning) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.0->qiskit<2.0,>=1.0->qiskit-machine-learning) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.3->qiskit<2.0,>=1.0->qiskit-machine-learning) (1.3.0)\n",
            "Downloading qiskit_machine_learning-0.8.4-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qiskit-1.4.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading symengine-0.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: symengine, scipy, qiskit, qiskit-machine-learning\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.3\n",
            "    Uninstalling scipy-1.16.3:\n",
            "      Successfully uninstalled scipy-1.16.3\n",
            "  Attempting uninstall: qiskit\n",
            "    Found existing installation: qiskit 2.2.3\n",
            "    Uninstalling qiskit-2.2.3:\n",
            "      Successfully uninstalled qiskit-2.2.3\n",
            "Successfully installed qiskit-1.4.5 qiskit-machine-learning-0.8.4 scipy-1.15.3 symengine-0.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install qiskit\n",
        "!pip install qiskit-aer\n",
        "!pip install qiskit-machine-learning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Imports and tunable parameters\n",
        "import os, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import OneClassSVM, SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Repro\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Parameters (set these to match your quantum notebook)\n",
        "PCA_COMPONENTS = 12        # same as quantum PCA_COMPONENTS\n",
        "ANOMALY_SUBSET_SIZE = 2000 # number of normal samples for one-class (same as quantum experiment)\n",
        "NUM_CLASSES = 4            # total classes including 'normal' (same as quantum)\n",
        "PER_CLASS = 400            # training samples per class for multiclass (same as quantum)\n",
        "TEST_SIZE = 1000            # number of test samples to evaluate\n",
        "SAVE_XTRAIN = \"X_train_pca.npy\"\n",
        "SAVE_XTEST  = \"X_test_pca.npy\"\n",
        "SAVE_DFTRAIN = \"df_train.pkl\"\n",
        "SAVE_DFTEST  = \"df_test.pkl\"\n",
        "\n",
        "print(\"Parameters:\", \"PCA=\", PCA_COMPONENTS, \"ANOMALY_SUBSET=\", ANOMALY_SUBSET_SIZE,\n",
        "      \"NUM_CLASSES=\", NUM_CLASSES, \"PER_CLASS=\", PER_CLASS, \"TEST_SIZE=\", TEST_SIZE)\n"
      ],
      "metadata": {
        "id": "eg2sit0PjvsT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "629eab6e-c3b8-4ac1-87eb-9590450130aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters: PCA= 12 ANOMALY_SUBSET= 2000 NUM_CLASSES= 4 PER_CLASS= 400 TEST_SIZE= 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Load raw NSL-KDD train/test files\n",
        "TRAIN_RAW = \"KDDTrain+.txt\"\n",
        "TEST_RAW  = \"KDDTest+.txt\"\n",
        "\n",
        "columns = [\n",
        "    'duration','protocol_type','service','flag','src_bytes','dst_bytes',\n",
        "    'land','wrong_fragment','urgent','hot','num_failed_logins','logged_in',\n",
        "    'num_compromised','root_shell','su_attempted','num_root','num_file_creations',\n",
        "    'num_shells','num_access_files','num_outbound_cmds','is_host_login','is_guest_login',\n",
        "    'count','srv_count','serror_rate','srv_serror_rate','rerror_rate','srv_rerror_rate',\n",
        "    'same_srv_rate','diff_srv_rate','srv_diff_host_rate','dst_host_count','dst_host_srv_count',\n",
        "    'dst_host_same_srv_rate','dst_host_diff_srv_rate','dst_host_same_src_port_rate',\n",
        "    'dst_host_srv_diff_host_rate','dst_host_serror_rate','dst_host_srv_serror_rate',\n",
        "    'dst_host_rerror_rate','dst_host_srv_rerror_rate','label','difficulty'\n",
        "]\n",
        "\n",
        "print(\"Loading raw files...\")\n",
        "df_train = pd.read_csv(TRAIN_RAW, names=columns)\n",
        "df_test  = pd.read_csv(TEST_RAW,  names=columns)\n",
        "print(\"Loaded. Train shape:\", df_train.shape, \"Test shape:\", df_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNNwMvs4zTK3",
        "outputId": "d6d09534-a1fd-4b50-ff10-62551ff448d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading raw files...\n",
            "Loaded. Train shape: (125973, 43) Test shape: (22544, 43)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Clean label column, inspect class counts\n",
        "# Make sure labels are strings and normalized\n",
        "df_train['label'] = df_train['label'].astype(str).str.strip().str.lower().str.rstrip('.')\n",
        "df_test['label']  = df_test['label'].astype(str).str.strip().str.lower().str.rstrip('.')\n",
        "\n",
        "print(\"Unique labels (train):\", df_train['label'].unique()[:30])\n",
        "print(\"Top label counts (train):\")\n",
        "print(df_train['label'].value_counts().head(12))\n",
        "\n",
        "n_normal_train = (df_train['label'] == 'normal').sum()\n",
        "print(\"Number of normal samples in train:\", n_normal_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDBqP6idzWRi",
        "outputId": "e828e577-b8f0-4310-b797-22abc51f2d3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels (train): ['normal' 'neptune' 'warezclient' 'ipsweep' 'portsweep' 'teardrop' 'nmap'\n",
            " 'satan' 'smurf' 'pod' 'back' 'guess_passwd' 'ftp_write' 'multihop'\n",
            " 'rootkit' 'buffer_overflow' 'imap' 'warezmaster' 'phf' 'land'\n",
            " 'loadmodule' 'spy' 'perl']\n",
            "Top label counts (train):\n",
            "label\n",
            "normal          67343\n",
            "neptune         41214\n",
            "satan            3633\n",
            "ipsweep          3599\n",
            "portsweep        2931\n",
            "smurf            2646\n",
            "nmap             1493\n",
            "back              956\n",
            "teardrop          892\n",
            "warezclient       890\n",
            "pod               201\n",
            "guess_passwd       53\n",
            "Name: count, dtype: int64\n",
            "Number of normal samples in train: 67343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: One-hot encode categorical columns, align train/test columns, scale numeric data\n",
        "# We'll save processed DataFrames and scaled arrays for reuse by Quantum notebook\n",
        "print(\"Preprocessing: one-hot encoding categorical columns and scaling.\")\n",
        "\n",
        "# Drop 'difficulty' column for features\n",
        "df_train_proc = df_train.drop(columns=['difficulty']).copy()\n",
        "df_test_proc  = df_test.drop(columns=['difficulty']).copy()\n",
        "\n",
        "# Detect categorical columns (object dtype) except 'label'\n",
        "cat_cols = df_train_proc.select_dtypes(include=['object']).columns.tolist()\n",
        "cat_cols = [c for c in cat_cols if c != 'label']\n",
        "print(\"Categorical columns to encode:\", cat_cols)\n",
        "\n",
        "# One-hot encode categorical columns\n",
        "df_train_proc = pd.get_dummies(df_train_proc, columns=cat_cols)\n",
        "df_test_proc  = pd.get_dummies(df_test_proc,  columns=cat_cols)\n",
        "\n",
        "# Align test columns to train columns\n",
        "df_test_proc = df_test_proc.reindex(columns=df_train_proc.columns, fill_value=0)\n",
        "\n",
        "# Extract labels and features\n",
        "y_train_full = df_train_proc['label'].astype(str).values\n",
        "y_test_full  = df_test_proc['label'].astype(str).values\n",
        "X_train_df = df_train_proc.drop(columns=['label']).copy()\n",
        "X_test_df  = df_test_proc.drop(columns=['label']).copy()\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_df)\n",
        "X_test_scaled  = scaler.transform(X_test_df)\n",
        "\n",
        "print(\"Preprocessing complete. Shapes:\", X_train_scaled.shape, X_test_scaled.shape)\n",
        "\n",
        "# Save processed DF's for quantum notebook to load easily\n",
        "df_train.to_pickle(SAVE_DFTRAIN)\n",
        "df_test.to_pickle(SAVE_DFTEST)\n",
        "print(\"Saved df_train/df_test pkl for reuse by Quantum notebook.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dyomcT7zZ5_",
        "outputId": "dd293823-184b-4a39-8121-a11da709eb6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing: one-hot encoding categorical columns and scaling.\n",
            "Categorical columns to encode: ['protocol_type', 'service', 'flag']\n",
            "Preprocessing complete. Shapes: (125973, 122) (22544, 122)\n",
            "Saved df_train/df_test pkl for reuse by Quantum notebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: PCA reduction to PCA_COMPONENTS and save arrays for Notebook B\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=PCA_COMPONENTS, random_state=SEED)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca  = pca.transform(X_test_scaled)\n",
        "\n",
        "# Save arrays for Quantum notebook to load\n",
        "np.save(SAVE_XTRAIN, X_train_pca)\n",
        "np.save(SAVE_XTEST, X_test_pca)\n",
        "print(\"PCA done. Shapes:\", X_train_pca.shape, X_test_pca.shape)\n",
        "print(\"Saved X_train_pca.npy and X_test_pca.npy for reuse.\")\n",
        "print(\"Explained variance ratio (sum):\", pca.explained_variance_ratio_.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W44pTfozeaT",
        "outputId": "d55a1afd-f007-45a2-95c3-4f2fe3428fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA done. Shapes: (125973, 12) (22544, 12)\n",
            "Saved X_train_pca.npy and X_test_pca.npy for reuse.\n",
            "Explained variance ratio (sum): 0.3136342833926865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Classical One-Class SVM baseline — train on the SAME normal subset you'll use for quantum\n",
        "print(\"Building One-Class SVM baseline using the same normal subset size:\", ANOMALY_SUBSET_SIZE)\n",
        "\n",
        "# Collect all normal samples from PCA data\n",
        "normal_mask = (y_train_full == 'normal')\n",
        "X_train_normal_all = X_train_pca[normal_mask]\n",
        "n_normals = X_train_normal_all.shape[0]\n",
        "print(\"Available normal samples:\", n_normals)\n",
        "\n",
        "# Choose subset for training (reproducible)\n",
        "subset_size = min(ANOMALY_SUBSET_SIZE, n_normals)\n",
        "np.random.seed(SEED)\n",
        "selected_idx_normals = np.random.choice(n_normals, subset_size, replace=False)\n",
        "X_train_subset_normal = X_train_normal_all[selected_idx_normals]\n",
        "\n",
        "# Prepare test set (same test selection strategy as quantum notebook — first TEST_SIZE)\n",
        "test_size = min(TEST_SIZE, X_test_pca.shape[0])\n",
        "X_test_eval = X_test_pca[:test_size]\n",
        "# make binary labels: 0=normal, 1=attack\n",
        "y_test_eval = (y_test_full[:test_size] != 'normal').astype(int)\n",
        "\n",
        "print(\"Train subset shape (normal):\", X_train_subset_normal.shape)\n",
        "print(\"Test eval shape:\", X_test_eval.shape, \"with\", np.sum(y_test_eval==1), \"attacks in test subset.\")\n",
        "\n",
        "# Train classical One-Class SVM (RBF)\n",
        "t0 = time.time()\n",
        "oc_classical = OneClassSVM(kernel='rbf', gamma='scale', nu=0.1)\n",
        "oc_classical.fit(X_train_subset_normal)\n",
        "t1 = time.time()\n",
        "\n",
        "# Predict and evaluate\n",
        "t2 = time.time()\n",
        "y_pred_c = oc_classical.predict(X_test_eval)   # +1 inlier, -1 outlier\n",
        "y_pred_c = np.where(y_pred_c == 1, 0, 1)\n",
        "t3 = time.time()\n",
        "\n",
        "print(f\"Classical One-Class SVM: train_time={t1-t0:.3f}s predict_time={t3-t2:.3f}s\")\n",
        "print(\"Accuracy (classical one-class):\", accuracy_score(y_test_eval, y_pred_c))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test_eval, y_pred_c))\n",
        "print(\"Classification report:\\n\", classification_report(y_test_eval, y_pred_c, zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21iSKcN7zjzc",
        "outputId": "7268ec94-3aac-439c-d2e1-ebf2b129fd41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building One-Class SVM baseline using the same normal subset size: 2000\n",
            "Available normal samples: 67343\n",
            "Train subset shape (normal): (2000, 12)\n",
            "Test eval shape: (1000, 12) with 548 attacks in test subset.\n",
            "Classical One-Class SVM: train_time=0.032s predict_time=0.013s\n",
            "Accuracy (classical one-class): 0.84\n",
            "Confusion matrix:\n",
            " [[404  48]\n",
            " [112 436]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.89      0.83       452\n",
            "           1       0.90      0.80      0.84       548\n",
            "\n",
            "    accuracy                           0.84      1000\n",
            "   macro avg       0.84      0.84      0.84      1000\n",
            "weighted avg       0.85      0.84      0.84      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Classical multiclass baseline – build balanced subset identical to quantum multiclass setup\n",
        "print(\"Preparing balanced multiclass training subset: NUM_CLASSES =\", NUM_CLASSES, \"PER_CLASS =\", PER_CLASS)\n",
        "\n",
        "labels_all = df_train['label'].astype(str).str.strip().str.lower().to_numpy()\n",
        "unique, counts = np.unique(labels_all, return_counts=True)\n",
        "label_counts = sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)\n",
        "print(\"Top labels (train):\", label_counts[:10])\n",
        "\n",
        "# Select classes: always include 'normal' then top attacks (same logic as Quantum)\n",
        "classes = ['normal'] if 'normal' in unique else []\n",
        "for lbl, _ in label_counts:\n",
        "    if lbl == 'normal': continue\n",
        "    if len(classes) >= NUM_CLASSES:\n",
        "        break\n",
        "    classes.append(lbl)\n",
        "classes = classes[:NUM_CLASSES]\n",
        "print(\"Selected classes for multiclass baseline:\", classes)\n",
        "\n",
        "# Build indices per class\n",
        "np.random.seed(SEED)\n",
        "indices_per_class = []\n",
        "for cls in classes:\n",
        "    cls_idxs = np.where(labels_all == cls)[0]\n",
        "    if len(cls_idxs) == 0:\n",
        "        raise RuntimeError(f\"No samples found for class '{cls}'\")\n",
        "    take = min(PER_CLASS, len(cls_idxs))\n",
        "    chosen = np.random.choice(cls_idxs, take, replace=False)\n",
        "    indices_per_class.append(chosen)\n",
        "\n",
        "train_idx_multi = np.concatenate(indices_per_class)\n",
        "X_train_multi = X_train_pca[train_idx_multi]\n",
        "y_train_multi = labels_all[train_idx_multi]\n",
        "\n",
        "print(\"Multiclass train shape:\", X_train_multi.shape)\n",
        "print(\"Per-class counts:\", Counter(y_train_multi))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m3pNzTKzoJI",
        "outputId": "d7a61ce1-cb7e-4ff7-e5f7-950be9af5cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing balanced multiclass training subset: NUM_CLASSES = 4 PER_CLASS = 400\n",
            "Top labels (train): [('normal', np.int64(67343)), ('neptune', np.int64(41214)), ('satan', np.int64(3633)), ('ipsweep', np.int64(3599)), ('portsweep', np.int64(2931)), ('smurf', np.int64(2646)), ('nmap', np.int64(1493)), ('back', np.int64(956)), ('teardrop', np.int64(892)), ('warezclient', np.int64(890))]\n",
            "Selected classes for multiclass baseline: ['normal', 'neptune', 'satan', 'ipsweep']\n",
            "Multiclass train shape: (1600, 12)\n",
            "Per-class counts: Counter({'normal': 400, 'neptune': 400, 'satan': 400, 'ipsweep': 400})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Train classical multiclass classifiers on the SAME training subset and evaluate on same test\n",
        "# Prepare test set (we choose first TEST_SIZE test samples; ensure those include many classes)\n",
        "test_n = min(TEST_SIZE, X_test_pca.shape[0])\n",
        "X_test_multi_eval = X_test_pca[:test_n]\n",
        "y_test_multi_eval = df_test['label'].astype(str).str.strip().str.lower().to_numpy()[:test_n]\n",
        "\n",
        "print(\"Test multi eval shape:\", X_test_multi_eval.shape, \"unique labels in test sample:\", np.unique(y_test_multi_eval)[:10])\n",
        "\n",
        "# 1) SVC (RBF)\n",
        "t0 = time.time()\n",
        "svc = SVC(kernel='rbf', gamma='scale', class_weight='balanced')\n",
        "svc.fit(X_train_multi, y_train_multi)\n",
        "t1 = time.time()\n",
        "y_pred_svc = svc.predict(X_test_multi_eval)\n",
        "t2 = time.time()\n",
        "print(f\"SVC train_time={t1-t0:.2f}s predict_time={t2-t1:.2f}s\")\n",
        "print(\"SVC Accuracy:\", accuracy_score(y_test_multi_eval, y_pred_svc))\n",
        "print(\"SVC Confusion Matrix (selected classes):\\n\", confusion_matrix(y_test_multi_eval, y_pred_svc, labels=classes))\n",
        "print(\"SVC Classification Report:\\n\", classification_report(y_test_multi_eval, y_pred_svc, labels=classes, zero_division=0))\n",
        "\n",
        "# 2) Random Forest\n",
        "t0 = time.time()\n",
        "rf = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=SEED)\n",
        "rf.fit(X_train_multi, y_train_multi)\n",
        "t1 = time.time()\n",
        "y_pred_rf = rf.predict(X_test_multi_eval)\n",
        "t2 = time.time()\n",
        "print(f\"RF train_time={t1-t0:.2f}s predict_time={t2-t1:.2f}s\")\n",
        "print(\"RF Accuracy:\", accuracy_score(y_test_multi_eval, y_pred_rf))\n",
        "print(\"RF Confusion Matrix (selected classes):\\n\", confusion_matrix(y_test_multi_eval, y_pred_rf, labels=classes))\n",
        "print(\"RF Classification Report:\\n\", classification_report(y_test_multi_eval, y_pred_rf, labels=classes, zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlHaEWz_zssd",
        "outputId": "01aae4c4-6f25-499e-edc2-6e83069fc7bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test multi eval shape: (1000, 12) unique labels in test sample: ['apache2' 'back' 'buffer_overflow' 'guess_passwd' 'httptunnel' 'ipsweep'\n",
            " 'mailbomb' 'mscan' 'multihop' 'named']\n",
            "SVC train_time=0.02s predict_time=0.01s\n",
            "SVC Accuracy: 0.659\n",
            "SVC Confusion Matrix (selected classes):\n",
            " [[406   1  41   4]\n",
            " [  0 215   0   0]\n",
            " [  1   0  32   0]\n",
            " [  0   0   0   6]]\n",
            "SVC Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.69      0.90      0.78       452\n",
            "     neptune       0.82      1.00      0.90       215\n",
            "       satan       0.35      0.97      0.51        33\n",
            "     ipsweep       0.11      1.00      0.20         6\n",
            "\n",
            "   micro avg       0.66      0.93      0.77       706\n",
            "   macro avg       0.49      0.97      0.60       706\n",
            "weighted avg       0.71      0.93      0.80       706\n",
            "\n",
            "RF train_time=1.49s predict_time=0.04s\n",
            "RF Accuracy: 0.684\n",
            "RF Confusion Matrix (selected classes):\n",
            " [[434   2  13   3]\n",
            " [  0 214   1   0]\n",
            " [  3   0  30   0]\n",
            " [  0   0   0   6]]\n",
            "RF Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.66      0.96      0.78       452\n",
            "     neptune       0.85      1.00      0.92       215\n",
            "       satan       0.48      0.91      0.62        33\n",
            "     ipsweep       0.19      1.00      0.32         6\n",
            "\n",
            "   micro avg       0.68      0.97      0.80       706\n",
            "   macro avg       0.55      0.97      0.66       706\n",
            "weighted avg       0.71      0.97      0.81       706\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Save selected indices and PCA arrays and a small summary CSV for reproducibility\n",
        "os.makedirs(\"classical_results\", exist_ok=True)\n",
        "\n",
        "# Save PCA arrays (already saved earlier) and training indices for reproducibility\n",
        "np.save(SAVE_XTRAIN, X_train_pca)\n",
        "np.save(SAVE_XTEST,  X_test_pca)\n",
        "np.save(\"classical_results/train_idx_multi.npy\", train_idx_multi)\n",
        "np.save(\"classical_results/selected_idx_normals.npy\", selected_idx_normals)\n",
        "\n",
        "# Save summary metrics to CSV\n",
        "summary = {\n",
        "    \"quantum_PCA_components\": PCA_COMPONENTS,\n",
        "    \"anomaly_subset_size\": subset_size,\n",
        "    \"multiclass_num_classes\": len(classes),\n",
        "    \"multiclass_per_class\": PER_CLASS,\n",
        "    \"test_size\": test_n\n",
        "}\n",
        "pd.Series(summary).to_csv(\"classical_results/summary_params.csv\")\n",
        "\n",
        "print(\"Saved artifacts to classical_results/. You can now run Notebook B (quantum) and it will load the same PCA arrays & DF pickles for 1:1 comparison.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGIDYgKPzwbs",
        "outputId": "caac1a81-accf-4845-a2a4-7c4a9591d7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifacts to classical_results/. You can now run Notebook B (quantum) and it will load the same PCA arrays & DF pickles for 1:1 comparison.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GSMJqgZaz63T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}